{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Q8_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelBFG/DL-studies/blob/master/%20Q8_7x7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFi1P1DT4FgS"
      },
      "source": [
        "# Solving a Maze with Deep Reinforcement Learning\n",
        "# Samuel Borges Ferreira Gomes\n",
        "# Prof. Fernando J. Von Zuben\n",
        "# FEEC/Unicamp - June/2020\n",
        "\n",
        "# Goal: Use Reinforcement Learning to solve a Qmaze\n",
        "# Based on https://www.samyzaf.com/ML/rl/qmaze.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjhi5kdG4FgT"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aDpGQDz4Fg0"
      },
      "source": [
        "from __future__ import print_function\n",
        "from time import sleep\n",
        "from IPython import display\n",
        "import pylab as pl\n",
        "import os, sys, time, datetime, json, random\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.optimizers import SGD , Adam, RMSprop\n",
        "from keras.layers.advanced_activations import PReLU\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upq4o54qV_DE"
      },
      "source": [
        "## Environment Description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQs4V1wY_jJS"
      },
      "source": [
        "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
        "rat_mark = 0.5      # The current rat cell will be painted by gray 0.5\n",
        "LEFT = 0\n",
        "UP = 1\n",
        "RIGHT = 2\n",
        "DOWN = 3\n",
        "\n",
        "# Actions dictionary\n",
        "actions_dict = {\n",
        "    LEFT: 'left',\n",
        "    UP: 'up',\n",
        "    RIGHT: 'right',\n",
        "    DOWN: 'down',\n",
        "}\n",
        "\n",
        "num_actions = len(actions_dict)\n",
        "\n",
        "# Exploration factor\n",
        "epsilon = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1gPgFfQAMu_"
      },
      "source": [
        "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
        "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
        "# rat = (row, col) initial rat position (defaults to (0,0))\n",
        "\n",
        "class Qmaze(object):\n",
        "    def __init__(self, maze, rat=(0,0)):\n",
        "        self._maze = np.array(maze)\n",
        "        nrows, ncols = self._maze.shape\n",
        "        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
        "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
        "        self.free_cells.remove(self.target)\n",
        "        if self._maze[self.target] == 0.0:\n",
        "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
        "        if not rat in self.free_cells:\n",
        "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
        "        self.reset(rat)\n",
        "\n",
        "    def reset(self, rat):\n",
        "        self.rat = rat\n",
        "        self.maze = np.copy(self._maze)\n",
        "        nrows, ncols = self.maze.shape\n",
        "        row, col = rat\n",
        "        self.maze[row, col] = rat_mark\n",
        "        self.state = (row, col, 'start')\n",
        "        self.min_reward = -0.5 * self.maze.size\n",
        "        self.total_reward = 0\n",
        "        self.visited = set()\n",
        "\n",
        "    def update_state(self, action):\n",
        "        nrows, ncols = self.maze.shape\n",
        "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
        "\n",
        "        if self.maze[rat_row, rat_col] > 0.0:\n",
        "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
        "\n",
        "        valid_actions = self.valid_actions()\n",
        "                \n",
        "        if not valid_actions:\n",
        "            nmode = 'blocked'\n",
        "        elif action in valid_actions:\n",
        "            nmode = 'valid'\n",
        "            if action == LEFT:\n",
        "                ncol -= 1\n",
        "            elif action == UP:\n",
        "                nrow -= 1\n",
        "            if action == RIGHT:\n",
        "                ncol += 1\n",
        "            elif action == DOWN:\n",
        "                nrow += 1\n",
        "        else:                  # invalid action, no change in rat position\n",
        "            mode = 'invalid'\n",
        "\n",
        "        # new state\n",
        "        self.state = (nrow, ncol, nmode)\n",
        "\n",
        "    def get_reward(self):\n",
        "        rat_row, rat_col, mode = self.state\n",
        "        nrows, ncols = self.maze.shape\n",
        "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
        "            return 1.0\n",
        "        if mode == 'blocked':\n",
        "            return self.min_reward - 1\n",
        "        if (rat_row, rat_col) in self.visited:\n",
        "            return -0.25\n",
        "        if mode == 'invalid':\n",
        "            return -0.75\n",
        "        if mode == 'valid':\n",
        "            return -0.04\n",
        "\n",
        "    def act(self, action):\n",
        "        self.update_state(action)\n",
        "        reward = self.get_reward()\n",
        "        self.total_reward += reward\n",
        "        status = self.game_status()\n",
        "        envstate = self.observe()\n",
        "        return envstate, reward, status\n",
        "\n",
        "    def observe(self):\n",
        "        canvas = self.draw_env()\n",
        "        envstate = canvas.reshape((1, -1))\n",
        "        return envstate\n",
        "\n",
        "    def draw_env(self):\n",
        "        canvas = np.copy(self.maze)\n",
        "        nrows, ncols = self.maze.shape\n",
        "        # clear all visual marks\n",
        "        for r in range(nrows):\n",
        "            for c in range(ncols):\n",
        "                if canvas[r,c] > 0.0:\n",
        "                    canvas[r,c] = 1.0\n",
        "        # draw the rat\n",
        "        row, col, valid = self.state\n",
        "        canvas[row, col] = rat_mark\n",
        "        return canvas\n",
        "\n",
        "    def game_status(self):\n",
        "        if self.total_reward < self.min_reward:\n",
        "            return 'lose'\n",
        "        rat_row, rat_col, mode = self.state\n",
        "        nrows, ncols = self.maze.shape\n",
        "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
        "            return 'win'\n",
        "\n",
        "        return 'not_over'\n",
        "\n",
        "    def valid_actions(self, cell=None):\n",
        "        if cell is None:\n",
        "            row, col, mode = self.state\n",
        "        else:\n",
        "            row, col = cell\n",
        "        actions = [0, 1, 2, 3]\n",
        "        nrows, ncols = self.maze.shape\n",
        "        if row == 0:\n",
        "            actions.remove(1)\n",
        "        elif row == nrows-1:\n",
        "            actions.remove(3)\n",
        "\n",
        "        if col == 0:\n",
        "            actions.remove(0)\n",
        "        elif col == ncols-1:\n",
        "            actions.remove(2)\n",
        "\n",
        "        if row>0 and self.maze[row-1,col] == 0.0:\n",
        "            actions.remove(1)\n",
        "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
        "            actions.remove(3)\n",
        "\n",
        "        if col>0 and self.maze[row,col-1] == 0.0:\n",
        "            actions.remove(0)\n",
        "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
        "            actions.remove(2)\n",
        "\n",
        "        return actions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UzUbweaWYRe"
      },
      "source": [
        "## Example of 8x8 maze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEgvv0QHAcqO"
      },
      "source": [
        "def show(qmaze):\n",
        "    plt.grid('on')\n",
        "    nrows, ncols = qmaze.maze.shape\n",
        "    ax = plt.gca()\n",
        "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
        "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "    canvas = np.copy(qmaze.maze)\n",
        "    for row,col in qmaze.visited:\n",
        "        canvas[row,col] = 0.6\n",
        "    rat_row, rat_col, _ = qmaze.state\n",
        "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
        "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
        "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PTTplH_Am9F"
      },
      "source": [
        "maze = np.array([\n",
        "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
        "    [ 1.,  0.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
        "    [ 1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.],\n",
        "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
        "    [ 1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
        "    [ 1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.],\n",
        "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
        "    [ 1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTO46kedArMF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "b760387f-9da0-4384-e751-04ef579da30a"
      },
      "source": [
        "qmaze = Qmaze(maze)\n",
        "canvas, reward, game_over = qmaze.act(DOWN)\n",
        "print(\"reward=\", reward)\n",
        "show(qmaze)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reward= -0.04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f84dacdf610>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QOkRblSA3IH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "d06e2ae9-48a3-4fa6-a0f6-f8aa2d92fde9"
      },
      "source": [
        "qmaze.act(DOWN)  # move down\n",
        "qmaze.act(RIGHT)  # move right\n",
        "qmaze.act(RIGHT)  # move right\n",
        "qmaze.act(RIGHT)  # move right\n",
        "qmaze.act(UP)  # move up\n",
        "show(qmaze)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f84da7bcc90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qj1TKfnA9qu"
      },
      "source": [
        "def play_game(model, qmaze, rat_cell):\n",
        "    qmaze.reset(rat_cell)\n",
        "    envstate = qmaze.observe()\n",
        "    while True:\n",
        "        prev_envstate = envstate\n",
        "        # get next action\n",
        "        q = model.predict(prev_envstate)\n",
        "        action = np.argmax(q[0])\n",
        "\n",
        "        # apply action, get rewards and new state\n",
        "        envstate, reward, game_status = qmaze.act(action)\n",
        "        if game_status == 'win':\n",
        "            return True\n",
        "        elif game_status == 'lose':\n",
        "            return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw53DdHQBKF0"
      },
      "source": [
        "def completion_check(model, qmaze):\n",
        "    for cell in qmaze.free_cells:\n",
        "        if not qmaze.valid_actions(cell):\n",
        "            return False\n",
        "        if not play_game(model, qmaze, cell):\n",
        "            return False\n",
        "    return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw-KCLD9HnbR"
      },
      "source": [
        "## Experience Replay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_M0KSnsHkc6"
      },
      "source": [
        "Experience replay is a technique where the agent's experience at each time step $e_{t}=(s_{t},a_{t},r_{t},s_{t+1})$ is stored in a data set $\\mathcal{D}=e_{1},e_{2},\\ldots,e_{N}$ into a replay memory. Instead of train from consecutives samples which leads to a inefficient learning due to their correlation, we then sample the memory randomly for a minibatch of experience. This tecnique is used to improve the optimization.\n",
        "\n",
        "The cell below implement Experience, the class in which we collect our game episodes (or game experiences) within a memory list.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm9WstgwBgjg"
      },
      "source": [
        "class Experience(object):\n",
        "    def __init__(self, model, max_memory=100, discount=0.95):\n",
        "        self.model = model\n",
        "        self.max_memory = max_memory\n",
        "        self.discount = discount\n",
        "        self.memory = list()\n",
        "        self.num_actions = model.output_shape[-1]\n",
        "\n",
        "    def remember(self, episode):\n",
        "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
        "        # memory[i] = episode\n",
        "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
        "        self.memory.append(episode)\n",
        "        if len(self.memory) > self.max_memory:\n",
        "            del self.memory[0]\n",
        "\n",
        "    def predict(self, envstate):\n",
        "        return self.model.predict(envstate)[0]\n",
        "\n",
        "    def get_data(self, data_size=10):\n",
        "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
        "        mem_size = len(self.memory)\n",
        "        data_size = min(mem_size, data_size)\n",
        "        inputs = np.zeros((data_size, env_size))\n",
        "        targets = np.zeros((data_size, self.num_actions))\n",
        "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
        "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
        "            inputs[i] = envstate\n",
        "            # There should be no target values for actions not taken.\n",
        "            targets[i] = self.predict(envstate)\n",
        "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
        "            Q_sa = np.max(self.predict(envstate_next))\n",
        "            if game_over:\n",
        "                targets[i, action] = reward\n",
        "            else:\n",
        "                # reward + gamma * max_a' Q(s', a')\n",
        "                targets[i, action] = reward + self.discount * Q_sa\n",
        "        return inputs, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZY3JPQuWo_m"
      },
      "source": [
        "## The Q-Training Algorithm for Qmaze"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzizregkIVus"
      },
      "source": [
        "Parameters:\n",
        "$$s_{t}\\in\\mathcal{S}\\rightarrow\\textrm{State}$$\n",
        "$$a_{t}\\in\\mathcal{A}\\rightarrow\\textrm{Action}$$\n",
        "$$\\pi_{i}\\rightarrow\\textrm{Policy}$$\n",
        "$$\\gamma\\rightarrow\\textrm{Discount Factor}$$\n",
        "$$\\textrm{Q-value } Q(s,a)\\rightarrow\\textrm{Distribution of reward given (state, action) pair}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw-1HBjjIF6z"
      },
      "source": [
        "Optimal Q-value:\n",
        "\n",
        "$$Q^{*}(s,a)=\\max_{\\pi}\\mathbb{E}\\left[\\sum_{t \\geq0}\\gamma^{t}r_{t}|s_{0}=s,a_{0}=a,\\pi\\right] $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jETRoicMIIV_"
      },
      "source": [
        "This satisfies the **Bellman** equation leading to the recursion\n",
        "\n",
        "$$Q^{*}(s,a)=\\mathbb{E}_{s'\\sim\\mathcal{E}}\\left[r+\\gamma\\max_{a'}Q^{*}(s',a')|(s,a)\\right] $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "702X8ShA80Fx"
      },
      "source": [
        "The Neural Network is used to approximate the action-value function\n",
        "\n",
        "$$Q(s,a;\\theta)\\approx Q^{*}(s,a) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr7e9OCp9OhN"
      },
      "source": [
        "The Loss Function used is the MSE following\n",
        "\n",
        "$$\\mathcal{L}_{i}(\\theta_{i})=\\mathbb{E}_{s,a\\sim p(\\cdot)}[(y_{i}-Q(s,a;\\theta_{i}))^{2}]$$\n",
        "\n",
        "where $$y_{i}=\\mathbb{E}_{s'\\sim\\mathcal{E}}\\left[r+\\gamma\\max_{a'}Q^{*}(s',a';\\theta_{i-1})|(s,a)\\right]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhOJYWGC-Buq"
      },
      "source": [
        "The loss function is iteratively trying to make the Bellman's equation $Q^{*}(s,a)$ close to the target value $y_{i}$. If this happens, the Q-function corresponds to the optimal $Q^*$ function leading to the optimal policy $\\pi^*$. Thus, the agent is suppose to take the best action in any state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e3lKfZ9_H9q"
      },
      "source": [
        "Gradient Update:\n",
        "\n",
        "$$\\nabla_{\\theta_{i}}\\mathcal{L}_{i}(\\theta_{i})=\\mathbb{E}_{s,a\\sim p(\\cdot);s'\\sim \\mathbb{E}}\\left[r+\\gamma\\max_{a'}Q(s',a';\\theta_{i-1})-Q(s,a;\\theta_{i})\\nabla_{\\theta}Q(s,a;\\theta_{i})\\right]$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QvK92JaBiL7"
      },
      "source": [
        "def qtrain(model, maze, **opt):\n",
        "    global epsilon\n",
        "    n_epoch = opt.get('n_epoch', 15000)\n",
        "    max_memory = opt.get('max_memory', 1000)\n",
        "    data_size = opt.get('data_size', 50)\n",
        "    weights_file = opt.get('weights_file', \"\")\n",
        "    name = opt.get('name', 'model')\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    # If you want to continue training from a previous model,\n",
        "    # just supply the h5 file name to weights_file option\n",
        "    if weights_file:\n",
        "        print(\"loading weights from file: %s\" % (weights_file,))\n",
        "        model.load_weights(weights_file)\n",
        "\n",
        "    # Construct environment/game from numpy array: maze (see above)\n",
        "    qmaze = Qmaze(maze)\n",
        "\n",
        "    # Initialize experience replay object\n",
        "    experience = Experience(model, max_memory=max_memory)\n",
        "\n",
        "    win_history = []   # history of win/lose game\n",
        "    n_free_cells = len(qmaze.free_cells)\n",
        "    hsize = qmaze.maze.size//2   # history window size\n",
        "    win_rate = 0.0\n",
        "    imctr = 1\n",
        "\n",
        "    for epoch in range(n_epoch):\n",
        "        loss = 0.0\n",
        "        rat_cell = random.choice(qmaze.free_cells)\n",
        "        qmaze.reset(rat_cell)\n",
        "        game_over = False\n",
        "\n",
        "        # get initial envstate (1d flattened canvas)\n",
        "        envstate = qmaze.observe()\n",
        "\n",
        "        n_episodes = 0\n",
        "        while not game_over:\n",
        "            valid_actions = qmaze.valid_actions()\n",
        "            if not valid_actions: break\n",
        "            prev_envstate = envstate\n",
        "            # Get next action\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = random.choice(valid_actions)\n",
        "            else:\n",
        "                action = np.argmax(experience.predict(prev_envstate))\n",
        "\n",
        "            # Apply action, get reward and new envstate\n",
        "            envstate, reward, game_status = qmaze.act(action)\n",
        "            if game_status == 'win':\n",
        "                win_history.append(1)\n",
        "                game_over = True\n",
        "            elif game_status == 'lose':\n",
        "                win_history.append(0)\n",
        "                game_over = True\n",
        "            else:\n",
        "                game_over = False\n",
        "\n",
        "            # Store episode (experience)\n",
        "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
        "            experience.remember(episode)\n",
        "            n_episodes += 1\n",
        "\n",
        "            # Train neural network model\n",
        "            inputs, targets = experience.get_data(data_size=data_size)\n",
        "            h = model.fit(\n",
        "                inputs,\n",
        "                targets,\n",
        "                epochs=8,\n",
        "                batch_size=16,\n",
        "                verbose=0,\n",
        "            )\n",
        "            loss = model.evaluate(inputs, targets, verbose=0)\n",
        "\n",
        "        if len(win_history) > hsize:\n",
        "            win_rate = sum(win_history[-hsize:]) / hsize\n",
        "    \n",
        "        dt = datetime.datetime.now() - start_time\n",
        "        t = format_time(dt.total_seconds())\n",
        "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
        "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
        "        # we simply check if training has exhausted all free cells and if in all\n",
        "        # cases the agent won\n",
        "        if win_rate > 0.9 : epsilon = 0.05\n",
        "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
        "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
        "            break\n",
        "\n",
        "    # Save trained model weights and architecture, this will be used by the visualization code\n",
        "    h5file = name + \".h5\"\n",
        "    json_file = name + \".json\"\n",
        "    model.save_weights(h5file, overwrite=True)\n",
        "    with open(json_file, \"w\") as outfile:\n",
        "        json.dump(model.to_json(), outfile)\n",
        "    end_time = datetime.datetime.now()\n",
        "    dt = datetime.datetime.now() - start_time\n",
        "    seconds = dt.total_seconds()\n",
        "    t = format_time(seconds)\n",
        "    print('files: %s, %s' % (h5file, json_file))\n",
        "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
        "    return seconds\n",
        "\n",
        "# This is a small utility for printing readable time strings:\n",
        "def format_time(seconds):\n",
        "    if seconds < 400:\n",
        "        s = float(seconds)\n",
        "        return \"%.1f seconds\" % (s,)\n",
        "    elif seconds < 4000:\n",
        "        m = seconds / 60.0\n",
        "        return \"%.2f minutes\" % (m,)\n",
        "    else:\n",
        "        h = seconds / 3600.0\n",
        "        return \"%.2f hours\" % (h,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8A-ESQIV2RJ"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W6tHgyyBzrr"
      },
      "source": [
        "def build_model(maze, lr=0.001):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
        "    model.add(PReLU())\n",
        "    model.add(Dense(maze.size))\n",
        "    model.add(PReLU())\n",
        "    model.add(Dense(num_actions))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdmDWgQTV5u9"
      },
      "source": [
        "# 7x7 Maze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUWPPKi6B5tj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "7549bbf1-8075-46c6-978f-ae87df32fc7e"
      },
      "source": [
        "maze =  np.array([\n",
        "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
        "    [ 1.,  1.,  1.,  0.,  0.,  1.,  0.],\n",
        "    [ 0.,  0.,  0.,  1.,  1.,  1.,  0.],\n",
        "    [ 1.,  1.,  1.,  1.,  0.,  0.,  1.],\n",
        "    [ 1.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
        "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
        "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
        "])\n",
        "\n",
        "qmaze = Qmaze(maze)\n",
        "show(qmaze)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f84da741610>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK9lCeTACAcy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "661665e8-d944-4fac-8f76-a58c95a2a5d2"
      },
      "source": [
        "model = build_model(maze)\n",
        "qtrain(model, maze, epochs=1000, max_memory=8*maze.size, data_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 000/14999 | Loss: 0.0446 | Episodes: 32 | Win count: 1 | Win rate: 0.000 | time: 52.1 seconds\n",
            "Epoch: 001/14999 | Loss: 0.0054 | Episodes: 105 | Win count: 1 | Win rate: 0.000 | time: 282.6 seconds\n",
            "Epoch: 002/14999 | Loss: 0.0023 | Episodes: 1 | Win count: 2 | Win rate: 0.000 | time: 284.8 seconds\n",
            "Epoch: 003/14999 | Loss: 0.0145 | Episodes: 3 | Win count: 3 | Win rate: 0.000 | time: 291.2 seconds\n",
            "Epoch: 004/14999 | Loss: 0.0013 | Episodes: 105 | Win count: 3 | Win rate: 0.000 | time: 8.78 minutes\n",
            "Epoch: 005/14999 | Loss: 0.0032 | Episodes: 3 | Win count: 4 | Win rate: 0.000 | time: 8.89 minutes\n",
            "Epoch: 006/14999 | Loss: 0.0052 | Episodes: 104 | Win count: 4 | Win rate: 0.000 | time: 12.68 minutes\n",
            "Epoch: 007/14999 | Loss: 0.0211 | Episodes: 104 | Win count: 4 | Win rate: 0.000 | time: 16.43 minutes\n",
            "Epoch: 008/14999 | Loss: 0.0158 | Episodes: 2 | Win count: 5 | Win rate: 0.000 | time: 16.51 minutes\n",
            "Epoch: 009/14999 | Loss: 0.0021 | Episodes: 2 | Win count: 6 | Win rate: 0.000 | time: 16.58 minutes\n",
            "Epoch: 010/14999 | Loss: 0.0039 | Episodes: 2 | Win count: 7 | Win rate: 0.000 | time: 16.65 minutes\n",
            "Epoch: 011/14999 | Loss: 0.0009 | Episodes: 105 | Win count: 7 | Win rate: 0.000 | time: 20.43 minutes\n",
            "Epoch: 012/14999 | Loss: 0.0015 | Episodes: 105 | Win count: 7 | Win rate: 0.000 | time: 24.20 minutes\n",
            "Epoch: 013/14999 | Loss: 0.0013 | Episodes: 109 | Win count: 7 | Win rate: 0.000 | time: 28.13 minutes\n",
            "Epoch: 014/14999 | Loss: 0.0473 | Episodes: 3 | Win count: 8 | Win rate: 0.000 | time: 28.24 minutes\n",
            "Epoch: 015/14999 | Loss: 0.0945 | Episodes: 10 | Win count: 9 | Win rate: 0.000 | time: 28.60 minutes\n",
            "Epoch: 016/14999 | Loss: 0.0096 | Episodes: 108 | Win count: 9 | Win rate: 0.000 | time: 32.52 minutes\n",
            "Epoch: 017/14999 | Loss: 0.0179 | Episodes: 35 | Win count: 10 | Win rate: 0.000 | time: 33.78 minutes\n",
            "Epoch: 018/14999 | Loss: 0.0029 | Episodes: 2 | Win count: 11 | Win rate: 0.000 | time: 33.85 minutes\n",
            "Epoch: 019/14999 | Loss: 0.0245 | Episodes: 2 | Win count: 12 | Win rate: 0.000 | time: 33.93 minutes\n",
            "Epoch: 020/14999 | Loss: 0.0129 | Episodes: 4 | Win count: 13 | Win rate: 0.000 | time: 34.07 minutes\n",
            "Epoch: 021/14999 | Loss: 0.0215 | Episodes: 1 | Win count: 14 | Win rate: 0.000 | time: 34.10 minutes\n",
            "Epoch: 022/14999 | Loss: 0.0344 | Episodes: 105 | Win count: 14 | Win rate: 0.000 | time: 37.91 minutes\n",
            "Epoch: 023/14999 | Loss: 0.0021 | Episodes: 2 | Win count: 15 | Win rate: 0.000 | time: 37.98 minutes\n",
            "Epoch: 024/14999 | Loss: 0.0342 | Episodes: 10 | Win count: 16 | Win rate: 0.625 | time: 38.34 minutes\n",
            "Epoch: 025/14999 | Loss: 0.0073 | Episodes: 1 | Win count: 17 | Win rate: 0.667 | time: 38.38 minutes\n",
            "Epoch: 026/14999 | Loss: 0.0400 | Episodes: 1 | Win count: 18 | Win rate: 0.667 | time: 38.42 minutes\n",
            "Epoch: 027/14999 | Loss: 0.0050 | Episodes: 3 | Win count: 19 | Win rate: 0.667 | time: 38.53 minutes\n",
            "Epoch: 028/14999 | Loss: 0.0247 | Episodes: 9 | Win count: 20 | Win rate: 0.708 | time: 38.86 minutes\n",
            "Epoch: 029/14999 | Loss: 0.0317 | Episodes: 101 | Win count: 20 | Win rate: 0.667 | time: 42.49 minutes\n",
            "Epoch: 030/14999 | Loss: 0.0128 | Episodes: 10 | Win count: 21 | Win rate: 0.708 | time: 42.85 minutes\n",
            "Epoch: 031/14999 | Loss: 0.0296 | Episodes: 94 | Win count: 22 | Win rate: 0.750 | time: 46.26 minutes\n",
            "Epoch: 032/14999 | Loss: 0.0029 | Episodes: 104 | Win count: 22 | Win rate: 0.708 | time: 50.02 minutes\n",
            "Epoch: 033/14999 | Loss: 0.0055 | Episodes: 57 | Win count: 23 | Win rate: 0.708 | time: 52.08 minutes\n",
            "Epoch: 034/14999 | Loss: 0.0019 | Episodes: 3 | Win count: 24 | Win rate: 0.708 | time: 52.19 minutes\n",
            "Epoch: 035/14999 | Loss: 0.0031 | Episodes: 2 | Win count: 25 | Win rate: 0.750 | time: 52.26 minutes\n",
            "Epoch: 036/14999 | Loss: 0.0034 | Episodes: 8 | Win count: 26 | Win rate: 0.792 | time: 52.55 minutes\n",
            "Epoch: 037/14999 | Loss: 0.0026 | Episodes: 18 | Win count: 27 | Win rate: 0.833 | time: 53.20 minutes\n",
            "Epoch: 038/14999 | Loss: 0.0031 | Episodes: 3 | Win count: 28 | Win rate: 0.833 | time: 53.31 minutes\n",
            "Epoch: 039/14999 | Loss: 0.0032 | Episodes: 15 | Win count: 29 | Win rate: 0.833 | time: 53.85 minutes\n",
            "Epoch: 040/14999 | Loss: 0.0019 | Episodes: 103 | Win count: 29 | Win rate: 0.833 | time: 57.59 minutes\n",
            "Epoch: 041/14999 | Loss: 0.0022 | Episodes: 4 | Win count: 30 | Win rate: 0.833 | time: 57.73 minutes\n",
            "Epoch: 042/14999 | Loss: 0.0026 | Episodes: 27 | Win count: 31 | Win rate: 0.833 | time: 58.71 minutes\n",
            "Epoch: 043/14999 | Loss: 0.0044 | Episodes: 4 | Win count: 32 | Win rate: 0.833 | time: 58.85 minutes\n",
            "Epoch: 044/14999 | Loss: 0.0015 | Episodes: 2 | Win count: 33 | Win rate: 0.833 | time: 58.92 minutes\n",
            "Epoch: 045/14999 | Loss: 0.0018 | Episodes: 2 | Win count: 34 | Win rate: 0.833 | time: 59.00 minutes\n",
            "Epoch: 046/14999 | Loss: 0.0025 | Episodes: 4 | Win count: 35 | Win rate: 0.875 | time: 59.14 minutes\n",
            "Epoch: 047/14999 | Loss: 0.0182 | Episodes: 18 | Win count: 36 | Win rate: 0.875 | time: 59.79 minutes\n",
            "Epoch: 048/14999 | Loss: 0.0018 | Episodes: 2 | Win count: 37 | Win rate: 0.875 | time: 59.87 minutes\n",
            "Epoch: 049/14999 | Loss: 0.0025 | Episodes: 3 | Win count: 38 | Win rate: 0.875 | time: 59.97 minutes\n",
            "Epoch: 050/14999 | Loss: 0.0149 | Episodes: 16 | Win count: 39 | Win rate: 0.875 | time: 60.56 minutes\n",
            "Epoch: 051/14999 | Loss: 0.0019 | Episodes: 4 | Win count: 40 | Win rate: 0.875 | time: 60.70 minutes\n",
            "Epoch: 052/14999 | Loss: 0.0088 | Episodes: 102 | Win count: 40 | Win rate: 0.833 | time: 64.39 minutes\n",
            "Epoch: 053/14999 | Loss: 0.0155 | Episodes: 5 | Win count: 41 | Win rate: 0.875 | time: 64.57 minutes\n",
            "Epoch: 054/14999 | Loss: 0.0017 | Episodes: 18 | Win count: 42 | Win rate: 0.875 | time: 65.22 minutes\n",
            "Epoch: 055/14999 | Loss: 0.0030 | Episodes: 2 | Win count: 43 | Win rate: 0.875 | time: 65.29 minutes\n",
            "Epoch: 056/14999 | Loss: 0.0043 | Episodes: 16 | Win count: 44 | Win rate: 0.917 | time: 65.87 minutes\n",
            "Epoch: 057/14999 | Loss: 0.0018 | Episodes: 34 | Win count: 45 | Win rate: 0.917 | time: 1.12 hours\n",
            "Epoch: 058/14999 | Loss: 0.0178 | Episodes: 2 | Win count: 46 | Win rate: 0.917 | time: 1.12 hours\n",
            "Epoch: 059/14999 | Loss: 0.0014 | Episodes: 12 | Win count: 47 | Win rate: 0.917 | time: 1.13 hours\n",
            "Epoch: 060/14999 | Loss: 0.0011 | Episodes: 19 | Win count: 48 | Win rate: 0.917 | time: 1.14 hours\n",
            "Epoch: 061/14999 | Loss: 0.0013 | Episodes: 28 | Win count: 49 | Win rate: 0.917 | time: 1.16 hours\n",
            "Epoch: 062/14999 | Loss: 0.0022 | Episodes: 34 | Win count: 50 | Win rate: 0.917 | time: 1.18 hours\n",
            "Epoch: 063/14999 | Loss: 0.0019 | Episodes: 33 | Win count: 51 | Win rate: 0.917 | time: 1.20 hours\n",
            "Epoch: 064/14999 | Loss: 0.0017 | Episodes: 8 | Win count: 52 | Win rate: 0.958 | time: 1.20 hours\n",
            "Epoch: 065/14999 | Loss: 0.0009 | Episodes: 28 | Win count: 53 | Win rate: 0.958 | time: 1.22 hours\n",
            "Epoch: 066/14999 | Loss: 0.0013 | Episodes: 27 | Win count: 54 | Win rate: 0.958 | time: 1.23 hours\n",
            "Epoch: 067/14999 | Loss: 0.0013 | Episodes: 28 | Win count: 55 | Win rate: 0.958 | time: 1.25 hours\n",
            "Epoch: 068/14999 | Loss: 0.0013 | Episodes: 8 | Win count: 56 | Win rate: 0.958 | time: 1.26 hours\n",
            "Epoch: 069/14999 | Loss: 0.0036 | Episodes: 24 | Win count: 57 | Win rate: 0.958 | time: 1.27 hours\n",
            "Epoch: 070/14999 | Loss: 0.0003 | Episodes: 27 | Win count: 58 | Win rate: 0.958 | time: 1.29 hours\n",
            "Epoch: 071/14999 | Loss: 0.0007 | Episodes: 20 | Win count: 59 | Win rate: 0.958 | time: 1.30 hours\n",
            "Epoch: 072/14999 | Loss: 0.0002 | Episodes: 4 | Win count: 60 | Win rate: 0.958 | time: 1.30 hours\n",
            "Epoch: 073/14999 | Loss: 0.0008 | Episodes: 22 | Win count: 61 | Win rate: 0.958 | time: 1.31 hours\n",
            "Epoch: 074/14999 | Loss: 0.0010 | Episodes: 11 | Win count: 62 | Win rate: 0.958 | time: 1.32 hours\n",
            "Epoch: 075/14999 | Loss: 0.0010 | Episodes: 3 | Win count: 63 | Win rate: 0.958 | time: 1.32 hours\n",
            "Epoch: 076/14999 | Loss: 0.0011 | Episodes: 6 | Win count: 64 | Win rate: 1.000 | time: 1.33 hours\n",
            "Epoch: 077/14999 | Loss: 0.0003 | Episodes: 17 | Win count: 65 | Win rate: 1.000 | time: 1.34 hours\n",
            "Reached 100% win rate at epoch: 77\n",
            "files: model.h5, model.json\n",
            "n_epoch: 77, max_mem: 392, data: 32, time: 1.34 hours\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4826.132255"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAdYLKR_W53J"
      },
      "source": [
        "## Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGMhhmbZZv57"
      },
      "source": [
        "### Rat starting next to the cheese"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj3EC2GBDkYu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "0e4168d7-e9dd-4bd4-94ba-0a0daf54550f"
      },
      "source": [
        "rat_cell = random.choice(qmaze.free_cells)\n",
        "qmaze.reset(rat_cell)\n",
        "envstate = qmaze.observe()\n",
        "game_status = 'lose'\n",
        "q_list = []\n",
        "while(game_status != 'win'):\n",
        "  q = model.predict(envstate)\n",
        "  q_list.append(q)\n",
        "  action = np.argmax(q[0])\n",
        "  # action = np.argmax(model.predict(envstate))\n",
        "  envstate, reward, game_status = qmaze.act(action)\n",
        "  show(qmaze)\n",
        "  display.clear_output(wait=True)\n",
        "  display.display(pl.gcf())\n",
        "  # plt.gca()\n",
        "  sleep(0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSfqugAGY7UX"
      },
      "source": [
        "#### Q-value for each state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzxW5UEKYZf4",
        "outputId": "e7e6b70d-8dcb-4c84-930b-4e8dbea682dc"
      },
      "source": [
        "print(q_list[0])\n",
        "print(q_list[1])\n",
        "print(q_list[2])\n",
        "print(q_list[3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.14843318 0.04860341 0.667436   0.15819493]]\n",
            "[[-0.30669457  0.15068975  0.8274695   0.04119867]]\n",
            "[[ 0.5532253  -0.00534523 -0.32635522  0.9289733 ]]\n",
            "[[ 0.4720667   0.02251636 -0.5350603   1.0036199 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me420ql7ZAeo"
      },
      "source": [
        "The structure of this vector given as follows: \\[LEFT UP RIGHT DOWN\\]\n",
        "\n",
        "The first move indicated by the first vector above should be maximum at its 3th position. Thus, the agent should move to the RIGHT.\n",
        "\n",
        "The second move indicated by the second vector above should be maximum at its 3th position. Thus, the agent should move to the RIGHT.\n",
        "\n",
        "The third move indicated by the third vector above should be maximum at its 4th position. Thus, the agent should move to the DOWN.\n",
        "\n",
        "The fourth move indicated by the fourth vector above should be maximum at its 4th position. Thus, the agent should move to the DOWN.\n",
        "\n",
        "Indeed, we can see that the optimal policy leads to RIGHT -> RIGHT -> DOWN -> DOWN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnUGziDxZzdk"
      },
      "source": [
        "### Rat starting far from the cheese"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "wuEDQJRtT7iH",
        "outputId": "5346ad2c-70b3-4f12-9107-34267d0c26e4"
      },
      "source": [
        "rat_cell = random.choice(qmaze.free_cells)\n",
        "qmaze.reset(rat_cell)\n",
        "envstate = qmaze.observe()\n",
        "game_status = 'lose'\n",
        "q_list = []\n",
        "while(game_status != 'win'):\n",
        "  q_list.append(q)\n",
        "  q = model.predict(envstate)\n",
        "  action = np.argmax(q[0])\n",
        "  # action = np.argmax(model.predict(envstate))\n",
        "  envstate, reward, game_status = qmaze.act(action)\n",
        "  show(qmaze)\n",
        "  display.clear_output(wait=True)\n",
        "  display.display(pl.gcf())\n",
        "  sleep(0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GM4EG82aJdA"
      },
      "source": [
        "#### Q-value for three chosen states\n",
        "\n",
        "The states chosen is located at (4,1), (4,4), (4,6) at the maze grid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAoND0SeURLg",
        "outputId": "09666e6a-e9ec-41d1-ffca-ab805486b456"
      },
      "source": [
        "print(q_list[1])\n",
        "print(q_list[7])\n",
        "print(q_list[17])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.61037314 -0.4461677  -0.2397319  -0.4854936 ]]\n",
            "[[-0.10409445 -0.50779194 -0.34459627 -0.30575278]]\n",
            "[[0.21655667 0.5817853  0.3542377  0.30314842]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvV2EL_ra4u-"
      },
      "source": [
        "The structure of this vector given as follows: \\[LEFT UP RIGHT DOWN\\]\n",
        "\n",
        "The Q-value at (4,1) grid state indicated by the first vector above should be maximum at its 3th position. Thus, the agent should move to the RIGHT.\n",
        "\n",
        "The Q-value at (4,4) grid state indicated by the second vector above should be maximum at its 1th position. Thus, the agent should move to the LEFT.\n",
        "\n",
        "The Q-value at (4,6) grid state indicated by the third vector above should be maximum at its 3th position. Thus, the agent should move to the RIGHT.\n",
        "\n",
        "Indeed, we can see that the optimal policy leads to these results."
      ]
    }
  ]
}